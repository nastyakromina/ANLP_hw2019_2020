{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Источник корпуса - научно-популярный онлайн-журнал Naked Science https://naked-science.ru/**\n",
    "\n",
    "Ключевые слова указаны как хэштеги до и после статьи"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 1. Создание мини-корпуса**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = open('Text1.txt', 'r', encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = f1.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "f2 = open('Text2.txt', 'r', encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2 = f2.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "f3 = open('Text3.txt', 'r', encoding = 'utf-8')\n",
    "f4 = open('Text4.txt', 'r', encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "t3 = f3.read()\n",
    "t4 = f4.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 2. Самостоятельная разметка ключевых слов**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Я бы выделила похожие ключевые слова, как авторы статей. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 3.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сначала достаём именные группы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading UDPipe model: done.\n"
     ]
    }
   ],
   "source": [
    "!C:/Users/Asus/Sem1_avtobreya/udpipe-1.2.0-bin/bin-win64/udpipe --input horizontal --output conllu \\\n",
    "--tokenize --tag --parse \\\n",
    "C:/Users/Asus/Sem1_avtobreya/russian-syntagrus-ud-2.4-190531.udpipe \\\n",
    "< Text1.txt > Text1.conllu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading UDPipe model: done.\n"
     ]
    }
   ],
   "source": [
    "!C:/Users/Asus/Sem1_avtobreya/udpipe-1.2.0-bin/bin-win64/udpipe --input horizontal --output conllu \\\n",
    "--tokenize --tag --parse \\\n",
    "C:/Users/Asus/Sem1_avtobreya/russian-syntagrus-ud-2.4-190531.udpipe \\\n",
    "< Text2.txt > Text2.conllu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading UDPipe model: done.\n"
     ]
    }
   ],
   "source": [
    "!C:/Users/Asus/Sem1_avtobreya/udpipe-1.2.0-bin/bin-win64/udpipe --input horizontal --output conllu \\\n",
    "--tokenize --tag --parse \\\n",
    "C:/Users/Asus/Sem1_avtobreya/russian-syntagrus-ud-2.4-190531.udpipe \\\n",
    "< Text3.txt > Text3.conllu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading UDPipe model: done.\n"
     ]
    }
   ],
   "source": [
    "!C:/Users/Asus/Sem1_avtobreya/udpipe-1.2.0-bin/bin-win64/udpipe --input horizontal --output conllu \\\n",
    "--tokenize --tag --parse \\\n",
    "C:/Users/Asus/Sem1_avtobreya/russian-syntagrus-ud-2.4-190531.udpipe \\\n",
    "< Text4.txt > Text4.conllu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.parse import DependencyGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trees(file):\n",
    "    trees = []\n",
    "\n",
    "    with open(file, encoding = 'utf-8') as f:\n",
    "        parsed_sents = f.read().split('\\n\\n')\n",
    "\n",
    "        for sent in parsed_sents:\n",
    "            tree = [line for line in sent.split('\\n') if line and line[0] != '#']\n",
    "            trees.append('\\n'.join(tree))\n",
    "    return trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "trees1 = trees('Text1.conllu')\n",
    "trees2 = trees('Text2.conllu')\n",
    "trees3 = trees('Text3.conllu')\n",
    "trees4 = trees('Text4.conllu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "_FILTER_RELS = ['punct', 'conj', 'parataxis']\n",
    "def get_subtree(nodes, node):\n",
    "    if not nodes[node]['deps']:\n",
    "        return [node]\n",
    "    else:\n",
    "        return [node] + [get_subtree(nodes, dep) for rel in nodes[node]['deps'] \n",
    "                         if rel not in _FILTER_RELS\n",
    "                         for dep in nodes[node]['deps'][rel]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(l):\n",
    "    flat = []\n",
    "    for el in l:\n",
    "        if not isinstance(el, list):\n",
    "            flat.append(el)\n",
    "        else:\n",
    "            flat += flatten(el)\n",
    "    return flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def results(trees):\n",
    "    result = []\n",
    "    for t in trees:\n",
    "        g = DependencyGraph(t, top_relation_label='root')\n",
    "        np_list = []\n",
    "        for n in g.nodes:\n",
    "            if g.nodes[n]['ctag'] == 'NOUN':\n",
    "                np = list(sorted(flatten(get_subtree(g.nodes, n))))\n",
    "                np_list.append(\n",
    "                    ' '.join([g.nodes[i]['word'] for i in np])\n",
    "                )\n",
    "        result.append(np_list)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "r1 = results(trees1)\n",
    "r2 = results(trees2)\n",
    "r3 = results(trees3)\n",
    "r4 = results(trees4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RAKE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-rake in c:\\users\\asus\\anaconda3\\lib\\site-packages (1.4.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install python-rake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import RAKE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = stopwords.words('russian')\n",
    "stop.append('это')\n",
    "stop.append('однако')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Инициализируем анализатор списком стоп-слов\n",
    "rake = RAKE.Rake(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Запускаем\n",
    "wiki_kw_list1 = rake.run(t1, maxWords=2, minFrequency=1)\n",
    "wiki_kw_list2 = rake.run(t2, maxWords=2, minFrequency=2)\n",
    "wiki_kw_list3 = rake.run(t3, maxWords=2, minFrequency=1)\n",
    "wiki_kw_list4 = rake.run(t4, maxWords=2, minFrequency=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('будут правдивы', 4.0),\n",
       " ('квантовом мире', 4.0),\n",
       " ('квантовый процесс', 4.0),\n",
       " ('завершения опыта', 4.0),\n",
       " ('состояние «ядро', 4.0),\n",
       " ('кот жив»', 4.0),\n",
       " ('таким образом', 4.0),\n",
       " ('признан живым', 4.0),\n",
       " ('жив кот', 4.0),\n",
       " ('экспериментатор сообщит', 4.0),\n",
       " ('исход эксперимента', 4.0),\n",
       " ('остальные друзья', 4.0),\n",
       " ('сценарий долго', 4.0),\n",
       " ('незначительным вплоть', 4.0),\n",
       " ('недавнего времени', 4.0),\n",
       " ('часлав брукнер', 4.0),\n",
       " ('формального доказательства', 4.0),\n",
       " ('друзья ждали', 4.0),\n",
       " ('гадали снаружи', 4.0),\n",
       " ('оно нарушится', 4.0),\n",
       " ('мысленный эксперимент', 4.0),\n",
       " ('реальном мире', 4.0),\n",
       " ('шести фотонов', 4.0),\n",
       " ('конце концов', 4.0),\n",
       " ('квантового мира', 4.0),\n",
       " ('«одной правды»', 4.0),\n",
       " ('категорию друзей', 3.5),\n",
       " ('каждый наблюдатель', 3.333333333333333),\n",
       " ('проводящих измерения', 3.333333333333333),\n",
       " ('друзей', 1.5),\n",
       " ('измерения', 1.3333333333333333),\n",
       " ('каждый', 1.3333333333333333),\n",
       " ('привыкли', 1.0),\n",
       " ('явление', 1.0),\n",
       " ('видеть', 1.0),\n",
       " ('впечатления', 1.0),\n",
       " ('нем', 1.0),\n",
       " ('эмоциональной', 1.0),\n",
       " ('просто', 1.0),\n",
       " ('оказывается', 1.0),\n",
       " ('верна', 1.0),\n",
       " ('котом', 1.0),\n",
       " ('добавив', 1.0),\n",
       " ('представим', 1.0),\n",
       " ('распалось', 1.0),\n",
       " ('лаборатории', 1.0),\n",
       " ('знает', 1.0),\n",
       " ('признают', 1.0),\n",
       " ('расскажут', 1.0),\n",
       " ('кота', 1.0),\n",
       " ('люди', 1.0),\n",
       " ('жизнью', 1.0),\n",
       " ('смертью', 1.0),\n",
       " ('отражает', 1.0),\n",
       " ('реальность', 1.0),\n",
       " ('направлении', 1.0),\n",
       " ('использована', 1.0),\n",
       " ('наблюдателей', 1.0),\n",
       " ('понятия', 1.0),\n",
       " ('рамки', 1.0),\n",
       " ('1964 году', 1.0),\n",
       " ('находящихся', 1.0),\n",
       " ('суммировать', 1.0),\n",
       " ('которых', 1.0),\n",
       " ('верен', 1.0),\n",
       " ('помимо', 1.0),\n",
       " ('фотона', 1.0),\n",
       " ('несмотря', 1.0),\n",
       " ('показали', 1.0),\n",
       " ('следовательно', 1.0),\n",
       " ('означает', 1.0)]"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_kw_list3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('времени', 1.3333333333333333), ('ассирия', 1.0), ('поэтому', 1.0)]"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_kw_list4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'рога'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#попробуем с именными группами \n",
    "#текст первый\n",
    "set([x[0] for x in wiki_kw_list1]) & set(flatten(r1)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'мозгу'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#текст второй\n",
    "set([x[0] for x in wiki_kw_list2]) & set(flatten(r2)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'«одной правды»',\n",
       " 'друзей',\n",
       " 'измерения',\n",
       " 'исход эксперимента',\n",
       " 'каждый наблюдатель',\n",
       " 'категорию друзей',\n",
       " 'кот жив»',\n",
       " 'кота',\n",
       " 'лаборатории',\n",
       " 'реальность'}"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set([x[0] for x in wiki_kw_list3]) & set(flatten(r3)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set([x[0] for x in wiki_kw_list4]) & set(flatten(r4)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ключевые слова получились не очень, попробуем токенизировать текст"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymorphy2 import MorphAnalyzer\n",
    "from pymorphy2.tokenizers import simple_word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = MorphAnalyzer()\n",
    "def normalize_text(text):\n",
    "    lemmas = []\n",
    "    for t in simple_word_tokenize(text):\n",
    "        lemmas.append(\n",
    "            m.parse(t)[0].normal_form\n",
    "        )\n",
    "    return ' '.join(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#текст первый\n",
    "norm_text1 = rake.run(normalize_text(t1), maxWords=3, minFrequency=2) #стало лучше после лемматизации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('конец хх век', 9.0)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_text1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#текст второй\n",
    "norm_text2 = rake.run(normalize_text(t2), maxWords=3, minFrequency=2) #стало лучше после лемматизации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('весь жизнь', 4.3),\n",
       " ('мозг', 2.3846153846153846),\n",
       " ('нейрон', 2.2857142857142856),\n",
       " ('пациент', 2.2),\n",
       " ('учёный', 2.0),\n",
       " ('случай', 1.875),\n",
       " ('луковица', 1.8333333333333333),\n",
       " ('мочь', 1.8),\n",
       " ('мужчина', 1.8),\n",
       " ('весь', 1.8),\n",
       " ('нейропластичность', 1.75),\n",
       " ('который', 1.7142857142857142),\n",
       " ('какой-', 1.3333333333333333),\n",
       " ('детство', 1.3333333333333333),\n",
       " ('система', 1.3333333333333333),\n",
       " ('возможно', 1.0),\n",
       " ('подозревать', 1.0),\n",
       " ('нос', 1.0),\n",
       " ('девушка', 1.0),\n",
       " ('полый', 1.0),\n",
       " ('ребёнок', 1.0),\n",
       " ('мозжечок', 1.0),\n",
       " ('например', 1.0)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_text2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_text3 = rake.run(normalize_text(t3), maxWords=3, minFrequency=2) #стало лучше после лемматизации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('признать кот живой', 7.821428571428571),\n",
       " ('кот', 2.25),\n",
       " ('весь', 1.75),\n",
       " ('каждый', 1.6666666666666667),\n",
       " ('верный', 1.3333333333333333),\n",
       " ('измерение', 1.3333333333333333),\n",
       " ('образ', 1.0)]"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_text3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_text4 = rake.run(normalize_text(t4), maxWords=3, minFrequency=2) #стало лучше после лемматизации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('орошаемый земледелие', 3.666666666666667),\n",
       " ('весь', 1.5),\n",
       " ('время', 1.3333333333333333),\n",
       " ('поэтому', 1.0),\n",
       " ('власть', 1.0),\n",
       " ('юг', 1.0)]"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_text4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TextRank**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.summarization import keywords as kw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#функция делает один большой массив\n",
    "def NP_result(result):\n",
    "    NP_result = []\n",
    "    for i in result:\n",
    "        for j in i:\n",
    "            NP_result.append(j)\n",
    "    return NP_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "NP_result1 = NP_result(r1)\n",
    "NP_result2 = NP_result(r2)\n",
    "NP_result3 = NP_result(r3)\n",
    "NP_result4 = NP_result(r4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_1 = \" \".join(NP_result1) \n",
    "t_2 = \" \".join(NP_result2) \n",
    "t_3 = \" \".join(NP_result3) \n",
    "t_4 = \" \".join(NP_result4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "TextRank_kw1 = kw(normalize_text(t_1), pos_filter=[], scores=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "TextRank_kw2 = kw(normalize_text(t_2), pos_filter=[], scores=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "TextRank_kw3 = kw(normalize_text(t_3), pos_filter=[], scores=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "TextRank_kw4 = kw(normalize_text(t_4), pos_filter=[], scores=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TF-IDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(min_df=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = vectorizer.fit_transform(NP_result1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = vectorizer.idf_\n",
    "tf_idf_dict1 = dict(zip(vectorizer.get_feature_names(), idf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "#отфильтруем по значением от большего к меньшему\n",
    "kw_tf_idf1 = {x: y for x, y in filter(lambda x: tf_idf_dict1[x[0]] == max(tf_idf_dict1.values()), tf_idf_dict1.items())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2 = vectorizer.fit_transform(NP_result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = vectorizer.idf_\n",
    "tf_idf_dict2 = dict(zip(vectorizer.get_feature_names(), idf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "kw_tf_idf2 = {x: y for x, y in filter(lambda x: tf_idf_dict2[x[0]] == max(tf_idf_dict2.values()), tf_idf_dict2.items())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "X3 = vectorizer.fit_transform(NP_result3)\n",
    "idf = vectorizer.idf_\n",
    "tf_idf_dict3 = dict(zip(vectorizer.get_feature_names(), idf))\n",
    "kw_tf_idf3 = {x: y for x, y in filter(lambda x: tf_idf_dict3[x[0]] == max(tf_idf_dict3.values()), tf_idf_dict3.items())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "X4 = vectorizer.fit_transform(NP_result4)\n",
    "idf = vectorizer.idf_\n",
    "tf_idf_dict4 = dict(zip(vectorizer.get_feature_names(), idf))\n",
    "kw_tf_idf4 = {x: y for x, y in filter(lambda x: tf_idf_dict4[x[0]] == max(tf_idf_dict4.values()), tf_idf_dict4.items())}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 4. Оценить точность, полноту, F-меру**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_1 = open('Text1_kw.txt', 'r', encoding = 'utf-8')\n",
    "KW1 = f_1.read()\n",
    "KW1_new = KW1.split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_2 = open('Text2_kw.txt', 'r', encoding = 'utf-8')\n",
    "KW2 = f_2.read()\n",
    "KW2_new = KW2.split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_3 = open('Text3_kw.txt', 'r', encoding = 'utf-8')\n",
    "KW3 = f_3.read()\n",
    "KW3_new = KW3.split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_4 = open('Text4_kw.txt', 'r', encoding = 'utf-8')\n",
    "KW4 = f_4.read()\n",
    "KW4_new = KW4.split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalized_kw(KW_new):\n",
    "    normalized_kw = []\n",
    "    for i in KW_new:\n",
    "        k = normalize_text(i)\n",
    "        normalized_kw.append(k)\n",
    "    return normalized_kw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_kw1 = normalized_kw(KW1_new)\n",
    "orig_kw2 = normalized_kw(KW2_new)\n",
    "orig_kw3 = normalized_kw(KW2_new)\n",
    "orig_kw4 = normalized_kw(KW3_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Точность RAKE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def m_kw(text):\n",
    "    m1 = []\n",
    "    for i in text:\n",
    "        word = i[0]\n",
    "        m1.append(word)\n",
    "    return m1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1_kw = m_kw(norm_text1)\n",
    "text2_kw = m_kw(norm_text2)\n",
    "text3_kw = m_kw(norm_text3)\n",
    "text4_kw = m_kw(norm_text4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_TPF(text_kw, orig_kw):\n",
    "    TP = 0\n",
    "    FN = 0\n",
    "    FP = 0\n",
    "\n",
    "    for i in text_kw:\n",
    "        for j in orig_kw:\n",
    "            if i == j:\n",
    "                TP += 1\n",
    "            FN = len(orig_kw) - TP\n",
    "            FP = len(text_kw) - TP\n",
    "    return TP, FN, FP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_TPF1 = count_TPF(text1_kw, orig_kw1)\n",
    "count_TPF2 = count_TPF(text2_kw, orig_kw2)\n",
    "count_TPF3 = count_TPF(text3_kw, orig_kw3)\n",
    "count_TPF4 = count_TPF(text4_kw, orig_kw4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_RAKE1 = count_TPF1[0]/(count_TPF1[0]+count_TPF1[2]) #точность для первого текста\n",
    "precision_RAKE2 = count_TPF2[0]/(count_TPF2[0]+count_TPF2[2]) #точность для второго текста\n",
    "precision_RAKE3 = count_TPF3[0]/(count_TPF3[0]+count_TPF3[2])\n",
    "precision_RAKE4 = count_TPF4[0]/(count_TPF4[0]+count_TPF4[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Точность для первого текста = 0.0\n",
      "Точность для второго текста = 0.043478260869565216\n",
      "Точность для третьего текста = 0.0\n",
      "Точность для четвертого текста = 0.0\n"
     ]
    }
   ],
   "source": [
    "print('Точность для первого текста = ' + str(precision_RAKE1))\n",
    "print('Точность для второго текста = ' + str(precision_RAKE2))\n",
    "print('Точность для третьего текста = ' + str(precision_RAKE3))\n",
    "print('Точность для четвертого текста = ' + str(precision_RAKE4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Полнота RAKE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_RAKE1 = count_TPF1[0]/(count_TPF1[0]+count_TPF1[1])\n",
    "recall_RAKE1 = count_TPF2[0]/(count_TPF2[0]+count_TPF2[1])\n",
    "recall_RAKE3 = count_TPF3[0]/(count_TPF3[0]+count_TPF3[1])\n",
    "recall_RAKE4 = count_TPF4[0]/(count_TPF4[0]+count_TPF4[1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Полнота для первого текста = 0.5\n",
      "Полнота для второго текста = 0.5\n",
      "Полнота для третьего текста = 0.0\n",
      "Полнота для четвертого текста = 0.0\n"
     ]
    }
   ],
   "source": [
    "print('Полнота для первого текста = ' + str(recall_RAKE1))\n",
    "print('Полнота для второго текста = ' + str(recall_RAKE2))\n",
    "print('Полнота для третьего текста = ' + str(recall_RAKE3))\n",
    "print('Полнота для четвертого текста = ' + str(recall_RAKE4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**F-мера RAKE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def F_measure(precision, recall):\n",
    "    F = 2*(precision*recall)/(precision+recall)\n",
    "    return F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "#для первого, третьего и четвертого текста невозможно посчитать F-меру, деление на 0\n",
    "F_measure2 = F_measure(precision_RAKE2, recall_RAKE2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-мера для второго текста = 0.08\n"
     ]
    }
   ],
   "source": [
    "print('F-мера для второго текста = ' + str(F_measure2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Точность TextRank**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1_kw_TextRank = m_kw(TextRank_kw1)\n",
    "text2_kw_TextRank = m_kw(TextRank_kw2)\n",
    "text3_kw_TextRank = m_kw(TextRank_kw3)\n",
    "text4_kw_TextRank = m_kw(TextRank_kw4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_TPF1_TextRank = count_TPF(text1_kw_TextRank, orig_kw1)\n",
    "count_TPF2_TextRank = count_TPF(text2_kw_TextRank, orig_kw2)\n",
    "count_TPF3_TextRank = count_TPF(text3_kw_TextRank, orig_kw3)\n",
    "count_TPF4_TextRank = count_TPF(text4_kw_TextRank, orig_kw4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_TextRank1 = count_TPF1_TextRank[0]/(count_TPF1_TextRank[0]+count_TPF1_TextRank[2]) #точность для первого текста\n",
    "precision_TextRank2 = count_TPF2_TextRank[0]/(count_TPF2_TextRank[0]+count_TPF2_TextRank[2]) #точность для второго текста\n",
    "precision_TextRank3 = count_TPF3_TextRank[0]/(count_TPF3_TextRank[0]+count_TPF3_TextRank[2]) #точность для первого текста\n",
    "precision_TextRank4 = count_TPF4_TextRank[0]/(count_TPF4_TextRank[0]+count_TPF4_TextRank[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Точность для первого текста = 0.05555555555555555\n",
      "Точность для второго текста = 0.0\n",
      "Точность для третьего текста = 0.0\n",
      "Точность для четвертого текста = 0.0\n"
     ]
    }
   ],
   "source": [
    "print('Точность для первого текста = ' + str(precision_TextRank1))\n",
    "print('Точность для второго текста = ' + str(precision_TextRank2))\n",
    "print('Точность для третьего текста = ' + str(precision_TextRank3))\n",
    "print('Точность для четвертого текста = ' + str(precision_TextRank4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Полнота TextRank**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_TextRank1 = count_TPF1_TextRank[0]/(count_TPF1_TextRank[0]+count_TPF1_TextRank[1])\n",
    "recall_TextRank2 = count_TPF2_TextRank[0]/(count_TPF2_TextRank[0]+count_TPF2_TextRank[1])\n",
    "recall_TextRank3 = count_TPF3_TextRank[0]/(count_TPF3_TextRank[0]+count_TPF3_TextRank[1])\n",
    "recall_TextRank4 = count_TPF4_TextRank[0]/(count_TPF4_TextRank[0]+count_TPF4_TextRank[1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Полнота для первого текста = 0.25\n",
      "Полнота для второго текста = 0.0\n",
      "Полнота для третьего текста = 0.0\n",
      "Полнота для четвертого текста = 0.0\n"
     ]
    }
   ],
   "source": [
    "print('Полнота для первого текста = ' + str(recall_TextRank1))\n",
    "print('Полнота для второго текста = ' + str(recall_TextRank2))\n",
    "print('Полнота для третьего текста = ' + str(recall_TextRank3))\n",
    "print('Полнота для четвертого текста = ' + str(recall_TextRank4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**F-мера TextRank**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "F_measure1_TextRank = F_measure(precision_TextRank1, recall_TextRank1)\n",
    "#для второго, третьего и четвертого текста невозможно посчитать F-меру, деление на 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-мера для первого текста = 0.0909090909090909\n"
     ]
    }
   ],
   "source": [
    "print('F-мера для первого текста = ' + str(F_measure1_TextRank))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Точность TF-IDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def kw_tf_idf(kw_tf_idf):\n",
    "    kw_tfif = []\n",
    "    for key in kw_tf_idf:\n",
    "        kw_tfif.append(key)\n",
    "    return(kw_tfif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1_kw_TFIDF = kw_tf_idf(kw_tf_idf1)\n",
    "text2_kw_TFIDF = kw_tf_idf(kw_tf_idf2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_TPF1_TFIDF = count_TPF(text1_kw_TFIDF, orig_kw1)\n",
    "count_TPF2_TFIDF = count_TPF(text2_kw_TFIDF, orig_kw2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_TFIDF1 = count_TPF1_TFIDF[0]/(count_TPF1_TFIDF[0]+count_TPF1_TFIDF[2]) #точность для первого текста\n",
    "precision_TFIDF2 = count_TPF2_TFIDF[0]/(count_TPF2_TFIDF[0]+count_TPF2_TFIDF[2]) #точность для второго текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Точность для первого текста = 0.0\n",
      "Точность для второго текста = 0.0\n"
     ]
    }
   ],
   "source": [
    "print('Точность для первого текста = ' + str(precision_TFIDF1))\n",
    "print('Точность для второго текста = ' + str(precision_TFIDF2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Полнота TF-IDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_TFIDF1 = count_TPF1_TFIDF[0]/(count_TPF1_TFIDF[0]+count_TPF1_TFIDF[1]) #точность для первого текста\n",
    "recall_TFIDF2 = count_TPF2_TFIDF[0]/(count_TPF2_TFIDF[0]+count_TPF2_TFIDF[1]) #точность для второго текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Полнота для первого текста = 0.0\n",
      "Полнота для второго текста = 0.0\n"
     ]
    }
   ],
   "source": [
    "print('Полнота для первого текста = ' + str(recall_TFIDF1))\n",
    "print('Полнота для второго текста = ' + str(recall_TFIDF2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 5. Описать ошибки автоматического выделения ключевых слов**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выделяются лишние слова, которые автор не выделяет как ключевые.\n",
    "\n",
    "Многие из них, действительно, лишние и не несут смысловой нагрузки, но одновременно с этим и не входят в список стоп-слов.\n",
    "Слово может часто встречаться в тексте, но не быть важным. Например, \"конец 20 века\" - не главная тема статьи, но об этом упомянается в тексте несколько раз.\n",
    "\n",
    "Чтобы решить эту проблему надо создать/доработать инструмент, который бы помимо стоп-слов ещё удалял какие-нибудь такие абстрактные вещи как время; методы, которыми пользовались авторы, о которых много упоминается, но они не являются темой статьи и т.д. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
